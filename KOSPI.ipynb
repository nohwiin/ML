{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "Bqm0fagNXVJ8"
      ],
      "authorship_tag": "ABX9TyOafdiFnVzNEwKCDkm7zSPw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nohwiin/ML/blob/master/KOSPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcYvk2VML8qb"
      },
      "source": [
        "# **Import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMBAz-gd4-PH"
      },
      "source": [
        "pip install pandas-datareader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRjdGMHoxN0P"
      },
      "source": [
        "import pandas_datareader as pdr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, date, timedelta\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import copy\n",
        "from pandas import Series"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8jCmq8YshYZ"
      },
      "source": [
        "drive.mount('/gdrive')\n",
        "\n",
        "%cd \"/gdrive/My Drive/KOSPI/Model\"\n",
        "model_path = \"/gdrive/My Drive/KOSPI/Model\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz7nrzm1L2kX"
      },
      "source": [
        "# **Make Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU5X41omH6VH"
      },
      "source": [
        "> Train : Val : Test = 6 : 2 : 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bBeQxZS66c1"
      },
      "source": [
        "start_train = datetime(2015,1,1)\n",
        "end_train = datetime(2018,12,31)\n",
        "\n",
        "start_test = datetime(2019,1,1)\n",
        "end_test = datetime(2019,12,31)\n",
        "\n",
        "df_train = pdr.get_data_yahoo(\"^KS11\", start_train, end_train)\n",
        "df_test = pdr.get_data_yahoo(\"^KS11\", start_test, end_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "givdlTkGnjM0"
      },
      "source": [
        "def make_dataset(data, label, window_size=20):\n",
        "    feature_list = []\n",
        "    label_list = []\n",
        "    for i in range(len(data) - window_size):\n",
        "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
        "        label_list.append(np.array(label.iloc[i+window_size]))\n",
        "    return np.array(feature_list), np.array(label_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqm0fagNXVJ8"
      },
      "source": [
        "# **Window_size Selection**\n",
        "---\n",
        "다른 외부 정보를 사용하기 앞서 Window_size를 결정해 주기위해 KOSPI만의 정보를 사용해 학습 진행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVjhyclvLbia"
      },
      "source": [
        "> [단기: 5, 10, 20][중기: 60][장기: 120, 240]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJxCbd7Cmnwy"
      },
      "source": [
        "window_size_list = [5, 10, 20, 60, 120, 240]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eual0z_gIDXr"
      },
      "source": [
        "> 정규화 가격지수를 반영하기 위해 Min-Max 방식이 아닌 Z-score 방식 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8D5Zs2zEhms"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scale_cols = [\"High\", \"Low\", \"Open\", \"Close\", \"Volume\", \"Adj Close\"]\n",
        "\n",
        "df_train_Normalized = scaler.fit_transform(df_train[scale_cols])\n",
        "df_train_Normalized = pd.DataFrame(df_train_Normalized)\n",
        "df_train_Normalized.columns = scale_cols\n",
        "\n",
        "df_test_Normalized = scaler.fit_transform(df_test[scale_cols])\n",
        "df_test_Normalized = pd.DataFrame(df_test_Normalized)\n",
        "df_test_Normalized.columns = scale_cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8l9DgxiLg5U"
      },
      "source": [
        "> 10일 채택"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb4EbXv83AEI"
      },
      "source": [
        "trained_features = [\"High\", \"Low\", \"Open\", \"Volume\", \"Adj Close\"]\n",
        "\n",
        "for window_size in window_size_list:\n",
        "  # train dataset\n",
        "  train_feature, train_label = make_dataset(df_train_Normalized[trained_features], df_train_Normalized[\"Close\"], window_size)\n",
        "\n",
        "  # train, validation set 생성\n",
        "  x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
        "\n",
        "  test_feature, test_label = make_dataset(df_test_Normalized[trained_features], df_test_Normalized[\"Close\"], window_size)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(16, input_shape=(train_feature.shape[1], train_feature.shape[2]), activation='relu', return_sequences=False))\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
        "  filename = os.path.join(model_path, 'tmp_checkpoint_'+str(window_size)+'.h5')\n",
        "  checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "  history = model.fit(x_train, y_train, epochs=200, batch_size=16, validation_data=(x_valid, y_valid), callbacks=[early_stop, checkpoint])\n",
        "  print(window_size, min(history.history['val_loss']))\n",
        "\n",
        "  # weight 로딩\n",
        "  model.load_weights(filename)\n",
        "\n",
        "  # 예측\n",
        "  pred = model.predict(test_feature)\n",
        "\n",
        "  plt.figure(figsize=(12, 9))\n",
        "  plt.plot(test_label, label='actual')\n",
        "  plt.plot(pred, label='prediction'+str(window_size))\n",
        "  plt.legend()\n",
        "  plt.title(window_size, loc=\"center\")\n",
        "  \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSuG4Jp8Xnzu"
      },
      "source": [
        "# **Parameter Selection**\n",
        "---\n",
        "변수선택법을 적용하기위해 SHAP를 사용하고자 했지만 버전상의 문제인지 LSTM에 적용이 안됨.  \n",
        "직접 학습해가며 전진선택법으로 파라미터 추가  \n",
        "파라미터는 무역이 주요한 한국의 특성상 주요 원자재 선물과 각국의 주요 경제지수 활용\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zo8BF-3esku"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "model_path = \"/gdrive/My Drive/KOSPI/Model/GSPC\"\n",
        "window_size = 10\n",
        "\n",
        "trained_features = [\"High\", \"Low\", \"Open\", \"Volume\", \"Adj Close\"]\n",
        "\n",
        "tmp_df_train = df_train.copy()\n",
        "tmp_df_test = df_test.copy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scale_cols = tmp_df_train.columns\n",
        "\n",
        "df_train_Normalized = scaler.fit_transform(tmp_df_train[scale_cols])\n",
        "df_train_Normalized = pd.DataFrame(df_train_Normalized)\n",
        "df_train_Normalized.columns = scale_cols\n",
        "\n",
        "df_test_Normalized = scaler.fit_transform(tmp_df_test[scale_cols])\n",
        "df_test_Normalized = pd.DataFrame(df_test_Normalized)\n",
        "df_test_Normalized.columns = scale_cols\n",
        "\n",
        "# train dataset\n",
        "train_feature, train_label = make_dataset(df_train_Normalized[trained_features], df_train_Normalized[\"Close\"], window_size)\n",
        "test_feature, test_label = make_dataset(df_test_Normalized[trained_features], df_test_Normalized[\"Close\"], window_size)\n",
        "\n",
        "# train, validation set 생성\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
        "  \n",
        "model = Sequential()\n",
        "model.add(LSTM(16, input_shape=(train_feature.shape[1], train_feature.shape[2]), activation='relu', return_sequences=False))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
        "filename = os.path.join(model_path, 'tmp_checkpoint_Original.h5')\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "history = model.fit(x_train, y_train, verbose=0, epochs=200, batch_size=16, validation_data=(x_valid, y_valid), callbacks=[early_stop, checkpoint])\n",
        "\n",
        "Original = min(history.history['val_loss'])\n",
        "print(Original)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP-e6zWECQ85"
      },
      "source": [
        ">KE=F를 포함했을때 가장 효과가 좋았음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqc-tmFIhosx"
      },
      "source": [
        "left_features = [\"^TWII\", \"^IPSA\", \"^TA125.TA\", \"SI=F\", \"HG=F\", \"CL=F\", \"CC=F\", \"^GSPC\", \"^IXIC\", \"^NYA\", \"^XAX\", \"^VIX\", \"^FCHI\", \"^N100\", \"^BFX\", \"^N225\", \"000001.SS\", \"399001.SZ\", \"^AXJO\", \"^STOXX50E\"]\n",
        "Selected_Futures = [\"KE=F\"]\n",
        "\n",
        "best_score_history = [Original]\n",
        "y_range = Original\n",
        "\n",
        "for feature in left_features:\n",
        "  compared_features = copy.deepcopy(Selected_Futures)\n",
        "  compared_features.append(feature)\n",
        "  tmp_df_train = df_train.copy()\n",
        "  tmp_df_test = df_test.copy()\n",
        "\n",
        "  tmp_df_train = tmp_df_train.merge(pdr.get_data_yahoo(compared_features, start_train, end_train)[\"Close\"], left_on='Date', right_on='Date', how='left')\n",
        "  tmp_df_train.fillna(method=\"ffill\", inplace=True)\n",
        "  tmp_df_train.fillna(method=\"bfill\", inplace=True)\n",
        "\n",
        "  tmp_df_test = tmp_df_test.merge(pdr.get_data_yahoo(compared_features, start_test, end_test)[\"Close\"], left_on='Date', right_on='Date', how='left')\n",
        "  tmp_df_test.fillna(method=\"ffill\", inplace=True)\n",
        "  tmp_df_test.fillna(method=\"bfill\", inplace=True)\n",
        "\n",
        "  scale_cols = tmp_df_train.columns\n",
        "\n",
        "  df_train_Normalized = scaler.fit_transform(tmp_df_train[scale_cols])\n",
        "  df_train_Normalized = pd.DataFrame(df_train_Normalized)\n",
        "  df_train_Normalized.columns = scale_cols\n",
        "\n",
        "  df_test_Normalized = scaler.fit_transform(tmp_df_test[scale_cols])\n",
        "  df_test_Normalized = pd.DataFrame(df_test_Normalized)\n",
        "  df_test_Normalized.columns = scale_cols\n",
        "\n",
        "  # train dataset\n",
        "  train_feature, train_label = make_dataset(df_train_Normalized[trained_features + compared_features], df_train_Normalized[\"Close\"], window_size)\n",
        "  test_feature, test_label = make_dataset(df_test_Normalized[trained_features + compared_features], df_test_Normalized[\"Close\"], window_size)\n",
        "\n",
        "  # train, validation set 생성\n",
        "  x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(LSTM(16, input_shape=(train_feature.shape[1], train_feature.shape[2]), activation='relu', return_sequences=False))\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
        "  filename = os.path.join(model_path, 'tmp_checkpoint_'+feature+'.h5')\n",
        "  checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "  history = model.fit(x_train, y_train, verbose=0, epochs=200, batch_size=16, validation_data=(x_valid, y_valid), callbacks=[early_stop, checkpoint])\n",
        "  best_score_history.append(min(history.history['val_loss']))\n",
        "\n",
        "  if min(history.history['val_loss']) < Original: \n",
        "    Original = min(history.history['val_loss'])\n",
        "\n",
        "x = np.arange(len(left_features) + 1)\n",
        "features = ['Original'] + left_features\n",
        "\n",
        "plt.figure(figsize=(50, 9))\n",
        "plt.bar(x, best_score_history)\n",
        "plt.xticks(x, features)\n",
        "plt.ylim(0, y_range)\n",
        "plt.show()  \n",
        "print(Original)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAmh231MxIMG"
      },
      "source": [
        "compared_features = copy.deepcopy(Selected_Futures)\n",
        "tmp_df_test = df_test.copy()\n",
        "\n",
        "tmp_df_test = tmp_df_test.merge(pdr.get_data_yahoo(compared_features, start_test, end_test)[\"Close\"], left_on='Date', right_on='Date', how='left')\n",
        "tmp_df_test.fillna(method=\"ffill\", inplace=True)\n",
        "tmp_df_test.fillna(method=\"bfill\", inplace=True)\n",
        "\n",
        "scale_cols = tmp_df_test.columns\n",
        "\n",
        "df_test_Normalized = scaler.fit_transform(tmp_df_test[scale_cols])\n",
        "df_test_Normalized = pd.DataFrame(df_test_Normalized)\n",
        "df_test_Normalized.columns = scale_cols\n",
        "\n",
        "test_feature, test_label = make_dataset(df_test_Normalized[trained_features + compared_features], df_test_Normalized[\"Close\"], window_size)\n",
        "  \n",
        "model = Sequential()\n",
        "model.add(LSTM(16, input_shape=(test_feature.shape[1], test_feature.shape[2]), activation='relu', return_sequences=False))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.load_weights(os.path.join(model_path, 'tmp_checkpoint_KE=F.h5'))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
        "filename = os.path.join(model_path, 'tmp_checkpoint_'+feature+'.h5')\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "pred = model.predict(test_feature)\n",
        "\n",
        "plt.figure(figsize=(12, 9))\n",
        "plt.plot(test_label, label='actual')\n",
        "plt.plot(pred, label='prediction')\n",
        "plt.legend()\n",
        "plt.title(window_size, loc=\"center\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPyzem9lyaJr"
      },
      "source": [
        "# **Final Test**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "966AqY1ODI5m"
      },
      "source": [
        "> 지난 10일간의 데이터를 기반으로 다음날의 종가 예상"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4K9l6m6z25Z"
      },
      "source": [
        "def make_dataset(data, window_size=20):\n",
        "    feature_list = []\n",
        "    label_list = []\n",
        "    for i in range(len(data) - window_size):\n",
        "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
        "    return np.array(feature_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma-15XzMycpo"
      },
      "source": [
        "window_size = 10\n",
        "\n",
        "start_test = date.today()-timedelta(days=30)\n",
        "end_test = date.today()\n",
        "\n",
        "compared_features = [\"KE=F\"]\n",
        "tmp_df_test = pdr.get_data_yahoo(\"^KS11\", start_test, end_test)\n",
        "\n",
        "tmp_df_test = tmp_df_test.merge(pdr.get_data_yahoo(compared_features, start_test, end_test)[\"Close\"], left_on='Date', right_on='Date', how='left')\n",
        "tmp_df_test.fillna(method=\"ffill\", inplace=True)\n",
        "tmp_df_test.fillna(method=\"bfill\", inplace=True)\n",
        "\n",
        "tmp_df_test = tmp_df_test.iloc[-11:]\n",
        "\n",
        "scale_cols = tmp_df_test.columns\n",
        "\n",
        "df_test_Normalized = scaler.fit_transform(tmp_df_test[scale_cols])\n",
        "df_test_Normalized = pd.DataFrame(df_test_Normalized)\n",
        "df_test_Normalized.columns = scale_cols\n",
        "\n",
        "test_feature = make_dataset(df_test_Normalized[trained_features + compared_features], window_size)\n",
        "  \n",
        "model = Sequential()\n",
        "model.add(LSTM(16, input_shape=(test_feature.shape[1], test_feature.shape[2]), activation='relu', return_sequences=False))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.load_weights(os.path.join(model_path, 'tmp_checkpoint_KE=F.h5'))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
        "filename = os.path.join(model_path, 'tmp_checkpoint_'+feature+'.h5')\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "pred = model.predict(test_feature)\n",
        "pred = sum(pred)\n",
        "pred_series = Series(pred, index=[11])\n",
        "\n",
        "plt.figure(figsize=(12, 9))\n",
        "plt.xticks(np.arange(0, 12), labels=['D-10', 'D-9', 'D-8', 'D-7', 'D-6', 'D-5', 'D-4', 'D-3', 'D-2', 'D-1', 'D-Day', 'D+1'])\n",
        "plt.plot(df_test_Normalized[\"Close\"].append(pred_series), label='prediction')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}