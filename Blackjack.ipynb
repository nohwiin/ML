{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Blackjack source code_real.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "X3zP4AKENFmz",
        "Uio4vwZTNFnQ",
        "tXBkEYrtNFnY",
        "g8Kg9UevNFne",
        "kP9iwGTGNFnm",
        "kjzBx00rNFnu",
        "WvvuYtwxNFn0",
        "Jz-wYhDtNFn8",
        "fc_H1j3sNFoE",
        "yR39kavPNFoN"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nohwiin/ML/blob/master/Blackjack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxDCwxJFNFmb",
        "colab_type": "text"
      },
      "source": [
        "# BlackJack : Monte-Carlo with Exploring Start\n",
        "----\n",
        "\n",
        "## 설명\n",
        "블랙잭 게임에서의 최적 Policy를 찾는 프로그램입니다. 본 프로그램에서 정의된 블랙잭 룰은 다음과 같습니다.\n",
        " - 하나의 카드 덱(조커를 제외한 52장의 카드)을 사용\n",
        " - 카드의 합을 계산할 시 A = 1 or 11, J,Q,K = 10으로 계산\n",
        " - 플레이어는 Hit 또는 Stick만을 할 수 있음\n",
        " - 카드의 합이 21 이하일 경우, 딜러와 플레이어 중 숫자가 더 높은 쪽이 승리.\n",
        " - 카드의 합이 21 초과한 딜러/플레이어는 패배\n",
        " - 카드의 합이 같을 경우 무승부\n",
        "\n",
        "본 프로그램에서는 최적 Policy를 찾기 위해 Monte-Carlo Control with Exploring Start[1] 알고리즘을 사용합니다. 최적 Policy를 찾기 위해, 본 프로그램은 다음과 같은 변수로 구성된 State로부터 탐색을 수행합니다.\n",
        " - 플레이어 카드의 합 : 12 ~ 21 사이의 정수\n",
        " - 딜러가 보여주는 카드의 숫자 : 1 ~ 10 사이의 정수\n",
        " - 플레이어가 현재 사용 가능한 Ace의 유무 : True / False\n",
        "\n",
        "위와 같이 구성된 State S에서 Action A를 선택했을 때의 기대 리턴(게임이 끝났을 때 얻을 것으로 기대되는 보상의 합)은 Q(S,A)입니다. 많은 에피소드를 경험할수혹 Agent는 Optimal Q(S,A)에 가까운 값을 학습하게 되며, 이로부터 최적 Policy를 찾을 수 있습니다.\n",
        "\n",
        "----\n",
        "\n",
        "## 0. Package\n",
        "먼저 본 프로그램에서 사용할 Package를 Import합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjdI_xMKNFmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "from matplotlib import colors\n",
        "import pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3zP4AKENFmz",
        "colab_type": "text"
      },
      "source": [
        "## 1. Class : Deck\n",
        "카드 덱 한 벌에 해당하는 클래스를 선언합니다. 조커를 제외한 52장의 카드로 이루어져 있으며, 각 카드는 정수로 표기됩니다. <br>\n",
        "A = 11, J, Q, K = 10으로 표현되며, 나머지 카드는 해당 카드에 적힌 숫자만큼의 값을 갖습니다.\n",
        "\n",
        "### 클래스 구성\n",
        "* Attributes\n",
        "   * card_deck : List, 52개의 정수로 이루어짐\n",
        "* Methods\n",
        "   * shuffle() : 카드 덱을 랜덤하게 섞음\n",
        "   * draw() : 카드 덱에서 한 장을 뽑아 리턴\n",
        "   * reset() : 카드 덱을 초기화함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3gLwNb5NFm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Deck(object):\n",
        "    \"\"\"\n",
        "    Deck : Card deck, which can be shuffled, drawn, and reset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        deck = [2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 11]\n",
        "        self.card_deck = deck * 4\n",
        "        self.shuffle()\n",
        "\n",
        "    def shuffle(self):\n",
        "        random.shuffle(self.card_deck)\n",
        "\n",
        "    def draw(self):\n",
        "        return self.card_deck.pop()\n",
        "\n",
        "    def reset(self):\n",
        "        deck = [2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 11]\n",
        "        self.card_deck = deck * 4\n",
        "        self.shuffle()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSFNGue8NFnE",
        "colab_type": "text"
      },
      "source": [
        "## 2. Class : Dealer\n",
        "다음으로 Dealer 클래스를 선언합니다.<br> \n",
        "Dealer 클래스는 강화학습에서의 Environment에 해당합니다. 플레이어의 Action을 받고, 그에 따른 다음 State와 Reward를 반환합니다.<br>\n",
        "딜러는 시작할 때 두 장의 카드를 받습니다. 플레이어의 차례가 끝난 후, 플레이어의 카드 합이 21을 넘지 않는다면 딜러는 다음과 같이 행동합니다.\n",
        " - 딜러 카드의 합이 17 미만일 때 : 카드 한 장을 추가로 뽑음 (Hit)\n",
        " - 딜러 카드의 합이 17 이상일 때 : 플레이어의 카드 합과 비교하여 승패 결정 (Stick)\n",
        "\n",
        "### 클래스 구성\n",
        "딜러 클래스는 다음과 같이 구성됩니다\n",
        " - Attributes\n",
        "     - hands : List, 딜러가 가지고 있는 카드들\n",
        "     - usable_ace : List, 딜러가 가지고 있는 A의 인덱스\n",
        " - Methods \n",
        "     - hit(deck) : 카드 덱에서 카드 한 장을 뽑아 hands에 추가\n",
        "     - show() : 딜러가 가진 카드 중 랜덤한 한 장의 값을 리턴\n",
        "     - calculate_sum() : 딜러가 가진 카드의 합을 리턴. 카드 합이 21 이상일 때 사용 가능한 Ace가 있다면 사용함.\n",
        "     - action() : 딜러의 Action - 합이 17 미만이면 hit, 이상이면 stick\n",
        "     - observation(action, agent, deck) : 플레이어의 action을 받아 종료 여부와 reward 리턴\n",
        "     - calculate_rewards(agent, deck) : 플레이어와 딜러의 승패 계산 및 reward 리턴\n",
        "     - reset() : 딜러 클래스의 모든 attribute 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRtwDXcuNFnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dealer(object):\n",
        "    \"\"\"\n",
        "    Dealer : 딜러 클래스\n",
        "    딜러는 두 장의 카드를 받고, 카드의 합이 16 이하이면 Hit, 17이상이면 Stick 함.\n",
        "    처음 두 장을 받았을 때 한 장의 카드를 랜덤하게 오픈함.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        hand : 딜러가 가진 카드\n",
        "        usable_ace : 딜러가 가진 카드 리스트 중 ace의 인덱스\n",
        "        natural : 두 장의 카드로 21이 되면 True, 아니면 False\n",
        "        \"\"\"\n",
        "        self.hands = list()\n",
        "        self.usable_ace = list()\n",
        "\n",
        "    def hit(self, deck: Deck):\n",
        "        \"\"\"\n",
        "        딜러의 Hit. 새로운 카드가 Ace라면 사용 가능한 Ace 리스트에 추가함\n",
        "        :param deck: Deck Object\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_card = deck.draw()\n",
        "        if new_card == 11:\n",
        "            self.usable_ace.append(len(self.hands))\n",
        "        self.hands.append(new_card)\n",
        "\n",
        "    def show(self):\n",
        "        \"\"\"\n",
        "        딜러가 가진 카드 중 하나를 랜덤하게 보여줌\n",
        "        :return: 딜러의 카드 중 랜덤한 카드 숫자\n",
        "        \"\"\"\n",
        "        card = random.choice(self.hands)\n",
        "        if card == 11:\n",
        "            card = 1\n",
        "        return card\n",
        "\n",
        "    def calculate_sum(self):\n",
        "        \"\"\"\n",
        "        딜러가 가진 카드의 합을 구함\n",
        "        21을 넘을 때 사용 가능한 Ace가 있으면 사용함\n",
        "        :return: 딜러 카드의 합\n",
        "        \"\"\"\n",
        "        sums = sum(self.hands)\n",
        "        if sums > 21 and len(self.usable_ace) > 0:\n",
        "            self.hands[self.usable_ace.pop()] = 1\n",
        "            sums = sum(self.hands)\n",
        "        return sums\n",
        "\n",
        "    def action(self, deck: Deck):\n",
        "        \"\"\"\n",
        "        딜러의 순서 때 딜러의 행동.\n",
        "        숫자의 합이 16 이하일 때는 Hit, 17 이상이면 Stick\n",
        "        :param deck:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            sums = self.calculate_sum()\n",
        "            if sums < 17:\n",
        "                self.hit(deck)\n",
        "            else:\n",
        "                return sums\n",
        "\n",
        "    def observation(self, action, agent, deck):\n",
        "        \"\"\"\n",
        "        플레이어의 Action을 받아, 그에 맞는 Observation과 Reward를 반환\n",
        "        :param action: agent 의 Action\n",
        "        :param agent: agent 클래스\n",
        "        :param deck: deck 클래스\n",
        "        :return: 에피소드 종료 여부, reward\n",
        "        \"\"\"\n",
        "        done = False\n",
        "        reward = 0\n",
        "        if action == True:  # Hit\n",
        "            agent.hit(deck)\n",
        "            if agent.calculate_sum() > 21:  #플레이어의 Hit으로 인해 카드 합이 21이 넘으면 즉시 종료\n",
        "                done = True\n",
        "                reward = -1\n",
        "        else:  # Stick\n",
        "            done = True\n",
        "            reward = self.calcuate_reward(agent, deck)\n",
        "\n",
        "        return done, reward\n",
        "\n",
        "    def calcuate_reward(self, agent, deck):\n",
        "        \"\"\"\n",
        "        플레이어가 Stick했을 때 딜러와의 카드 비교 수행\n",
        "        :param agent:\n",
        "        :param deck:\n",
        "        :return: Reward\n",
        "        \"\"\"\n",
        "        agent_sum = agent.calculate_sum()           # 플레이어의 카드 합 계산\n",
        "        if agent_sum > 21:                          # 플레이어의 Bust (패)\n",
        "            return -1\n",
        "\n",
        "        dealer_sum = self.action(deck)              # 딜러의 카드 합 계산\n",
        "        if dealer_sum > 21:                         # 딜러가 Bust (승)\n",
        "            return 1\n",
        "        if dealer_sum > agent_sum:                  # 딜러의 카드 합 > 플레이어 합 (패)\n",
        "            return -1\n",
        "        if dealer_sum < agent_sum:                  # 딜러의 카드 합 < 플레이어 합 (승)\n",
        "            return 1\n",
        "        return 0                                   # 딜러의 카드 합 == 플레이어의 합 (무)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        딜러 초기화 (새로운 에피소드 시작을 위해)\n",
        "        \"\"\"\n",
        "        self.hands = list()\n",
        "        self.usable_ace = list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Uio4vwZTNFnQ",
        "colab_type": "text"
      },
      "source": [
        "## 3. Class : Agent\n",
        "Agent 클래스는 플레이어에 해당하며, 최적 Policy를 학습하는 주체가 됩니다. Agent는 여러 에피소드들로부터 Q값(기대 Return)을 계산하고, 다음 에피소드 수행 시 해당 State에서 Q값이 높은 Action을 선택하는 Greedy Policy를 따릅니다. \n",
        "\n",
        "### 클래스 구성\n",
        "\n",
        "Agent의 구성은 다음과 같습니다.\n",
        "\n",
        " - Attributes\n",
        "     - hands : List, Agent가 가지고 있는 카드들\n",
        "     - usable_ace : List, Agent가 가지고 있는 A의 인덱스\n",
        "     - Q_table : Dictionary, 기대 Return값과 방문 횟수를 저장\n",
        " - Methods \n",
        "     - hit(deck) : 카드 덱에서 카드 한 장을 뽑아 hands에 추가\n",
        "     - calculate_sum() : Agent가 가진 카드의 합을 리턴. 카드 합이 21 이상일 때 사용 가능한 Ace가 있다면 사용함.\n",
        "     - random_action() : 현재 State에 상관없이 랜덤하게 Action을 리턴\n",
        "     - policy(state) : 현재 State에서 가능한 Action 중, Q_table의 값이 큰 Action을 리턴\n",
        "     - update_qval(episode) : 에피소드로부터 평균 리턴을 계산하여 Q_table을 업데이트\n",
        "     - reset() : Agent 클래스의 모든 attribute 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmpTFOO8NFnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        hand : 플레이어의 카드\n",
        "        usable_ace : 사용 가능한 ace 리스트\n",
        "        Q_table : q(s,a) 값을 저장할 딕셔너리\n",
        "        \"\"\"\n",
        "        self.hands = list()\n",
        "        self.usable_ace = list()\n",
        "        self.Q_table = dict()\n",
        "\n",
        "    def hit(self, deck: Deck):\n",
        "        \"\"\"\n",
        "        덱에서 새로운 카드를 뽑음\n",
        "        :param deck: Deck for draw a card\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        new_card = deck.draw()\n",
        "        if new_card == 11:\n",
        "            self.usable_ace.append(len(self.hands))\n",
        "        self.hands.append(new_card)\n",
        "\n",
        "    def calculate_sum(self):\n",
        "        \"\"\"\n",
        "        플레이어가 가진 카드의 합을 구함.\n",
        "        21을 넘을 때 사용 가능한 ace가 있으면 사용함\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        sums = sum(self.hands)\n",
        "        if sums > 21 and len(self.usable_ace) > 0:\n",
        "            self.hands[self.usable_ace.pop()] = 1\n",
        "            sums = sum(self.hands)\n",
        "        return sums\n",
        "\n",
        "    def random_action(self):\n",
        "        \"\"\"\n",
        "        랜덤하게 행동\n",
        "        True = hit, False = stick\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return random.choice([True, False])\n",
        "\n",
        "    def policy(self, state):\n",
        "        \"\"\"\n",
        "        Agent의 policy 함수.\n",
        "        e의 확률로 랜덤 행동을 하며, 그 외에는 현재 state에서 큰 q(s,a)값을 갖는 action을 선택함\n",
        "        :param state: Agent에게 주어진 state\n",
        "        :return: agent의 action을 반환 , True = hit and False = stick\n",
        "        \"\"\"\n",
        "        # Q_table에서 현재 state-action에 대해 값이 존재하는지 검사함\n",
        "        for action in (True, False):\n",
        "            if (state, action) not in self.Q_table.keys():  # Q_table에 값이 없으면 0으로 초기화\n",
        "                self.Q_table[(state, action)] = [0, 0]      # (mean return, visit count)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        # q값이 큰 action 선택\n",
        "        if self.Q_table[(state, True)] > self.Q_table[(state, False)]:\n",
        "            return True     # Hit\n",
        "        elif self.Q_table[(state, True)] == self.Q_table[(state, False)]:   # q값이 같으면 무작위추출\n",
        "            return self.random_action()\n",
        "        else:\n",
        "            return False    # Stick\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Agent를 리셋함\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        self.hands = list()\n",
        "        self.usable_ace = list()\n",
        "\n",
        "    def update_qval(self, episode):\n",
        "        \"\"\"\n",
        "        에피소드(한 번의 게임)으로부터 Q_table 을 업데이트함\n",
        "        Q 테이블에 없는 state-action 쌍이 나오면 새로 생성\n",
        "        Q 테이블에 state-action 쌍이 존재한다면 Incremental mean 적용하여 업데이트\n",
        "        :param episode: Episode generated from environment\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        total_return = 0\n",
        "        for state, action, reward in episode[::-1]:     # 에피소드의 뒤에서부터 (역순)\n",
        "            total_return += reward                       # return Gt 계산\n",
        "            if (state, action) not in self.Q_table.keys():  # state-action 쌍이 없다면\n",
        "                self.Q_table[(state, action)] = [total_return, 1]   # 새롭게 엔트리 생성 (Gt, count)\n",
        "\n",
        "            else:  #이미 존재하는 state-action 쌍이면 Incremental mean 적용\n",
        "                prev_val = self.Q_table[(state, action)][0]         # 이전의 평균 return\n",
        "                count = self.Q_table[(state, action)][1] + 1        # count 증가\n",
        "                mean = prev_val + (total_return - prev_val) / count # 평균 계산 : Incremental Mean 적용\n",
        "                self.Q_table[(state, action)] = [mean, count]  # 업데이트"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXBkEYrtNFnY",
        "colab_type": "text"
      },
      "source": [
        "## 4. Class : MonteCarlo\n",
        "Monte-Carlo control with Exploring Start를 구현한 클래스입니다. 에피소드를 생성할 수 있고, 생성된 에피소드로부터 에이전트를 학습시킵니다.\n",
        "에피소드 생성 시 Exploring Start를 위해 에피소드의 첫 번째 State에서는 무작위로 Action을 선택하며, 이후에는 에이전트의 Policy를 따라 Action을 선택합니다.\n",
        "\n",
        "### 클래스 구성\n",
        "MonteCarlo 클래스는 다음과 같이 구성됩니다.\n",
        " - Attribute\n",
        "     - 없음\n",
        " - Methods \n",
        "     - generate_episode(dealer, agent, deck) : 딜러, 플레이어, 카드 덱을 이용해 한 번의 블랙잭 게임을 진행하고 에피소드를 반환합니다.\n",
        "     - train(dealer, agent, deck, it, verbose) : 지정된 이터레이션 수만큼의 에피소드로부터 플레이어를 학습시킵니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8xFGFtaNFnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MonteCarlo(object):\n",
        "    def generate_episode(self, dealer: Dealer, agent: Agent, deck: Deck):\n",
        "        \"\"\"\n",
        "        하나의 에피소드(게임)를 생성함\n",
        "        :param dealer:\n",
        "        :param agent:\n",
        "        :param deck:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        \n",
        "        # 카드 덱, 딜러, Agent를 초기화\n",
        "        deck.reset()\n",
        "        dealer.reset()\n",
        "        agent.reset()\n",
        "        agent.hit(deck)\n",
        "        agent.hit(deck)\n",
        "        dealer.hit(deck)\n",
        "        dealer.hit(deck)\n",
        "\n",
        "        done = False    # 에피소드의 종료 여부\n",
        "\n",
        "        episode = list()    # 에피소드\n",
        "\n",
        "        while not done:\n",
        "            # 에피소드가 끝날 때까지 State, Action, Reward를 생성\n",
        "            sums = agent.calculate_sum()\n",
        "            if sums < 12:\n",
        "                agent.hit(deck)\n",
        "                continue\n",
        "\n",
        "            state = (sums, bool(agent.usable_ace), dealer.show())\n",
        "\n",
        "            ########   Exploring Start ~!!!!!!!!! : \n",
        "            if len(episode) == 0:       # 첫번째 State 일 때는 무작위 Action 선택\n",
        "                action =agent.random_action()\n",
        "            else:                       # 그 외에는 Q 테이블에서 큰 값을 갖는 Action 선택\n",
        "                action = agent.policy(state)\n",
        "            \n",
        "            done, reward = dealer.observation(action, agent, deck)  # 에피소드 종료 여부, Reward 계산\n",
        "            \n",
        "            # 생성된 State, Action, Reward를 에피소드에 추가\n",
        "            episode.append([state, action, reward])\n",
        "\n",
        "        return episode\n",
        "\n",
        "    def train(self, dealer: Dealer, agent: Agent, deck: Deck, it=10000, verbose=True):\n",
        "        count = 0\n",
        "        win = 0\n",
        "        loss = 0\n",
        "        draw = 0\n",
        "        total_win = 0\n",
        "        total_loss = 0\n",
        "        total_draw = 0\n",
        "        result = str()\n",
        "        for i in range(it):\n",
        "            count += 1\n",
        "            episode = self.generate_episode(dealer, agent, deck)\n",
        "            agent.update_qval(episode)\n",
        "\n",
        "            if episode[-1][-1] == 1:\n",
        "                win += 1\n",
        "            elif episode[-1][-1] == 0:\n",
        "                draw += 1\n",
        "            else:\n",
        "                loss += 1\n",
        "\n",
        "            if count % 1000 == 0 and verbose == True:\n",
        "                total_win += win\n",
        "                total_loss += loss\n",
        "                total_draw += draw\n",
        "\n",
        "                print(\"========== Training : Episode \", count, \" ===========\")\n",
        "                print(\"Recent 1000 games win rate :{:.3f}%\".format(win / (win + loss) * 100))\n",
        "                print(\" -- 1000 Games WIN :\", win, \"DRAW :\", draw, \"LOSS :\", loss)\n",
        "                print(\"Total win rate : {:.3f}%\".format(total_win / (total_win + total_loss) * 100))\n",
        "                print(\" -- TOTAL Games WIN :\", total_win, \"DRAW :\", total_draw, \"LOSS :\", total_loss)\n",
        "\n",
        "                win = 0\n",
        "                loss = 0\n",
        "                draw = 0\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "g8Kg9UevNFne",
        "colab_type": "text"
      },
      "source": [
        "## 5. Function : plot_q_val(agent, usable_ace)\n",
        "Q값을 시각화하기 위한 함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gO4BLFxNFnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_q_val(agent: Agent, usable_ace=True):\n",
        "    fig = plt.figure()\n",
        "    ax = Axes3D(fig)\n",
        "\n",
        "    hands = set()\n",
        "    dealer_show = set()\n",
        "\n",
        "    for state, action in sorted(agent.Q_table.keys()):\n",
        "        hands.add(state[0])\n",
        "        dealer_show.add(state[2])\n",
        "\n",
        "    Z_list = list()\n",
        "\n",
        "    l = list()\n",
        "\n",
        "    for d in dealer_show:\n",
        "        l = []\n",
        "        for hand in hands:\n",
        "            if ((hand, usable_ace, d), True) not in agent.Q_table.keys():\n",
        "                agent.Q_table[((hand, usable_ace, d), True)] = [0, 0]\n",
        "            if ((hand, usable_ace, d), False) not in agent.Q_table.keys():\n",
        "                agent.Q_table[((hand, usable_ace, d), False)] = [0, 0]\n",
        "            v_val = max([agent.Q_table[((hand, usable_ace, d), True)][0], agent.Q_table[((hand, usable_ace, d), False)][0]])\n",
        "            l.append(v_val)\n",
        "        Z_list.append(l)\n",
        "\n",
        "    Y = np.array(list(hands))\n",
        "    X = np.array(list(dealer_show))\n",
        "    if usable_ace:\n",
        "        X, Y = np.meshgrid(X, Y)\n",
        "    else:\n",
        "        X, Y = np.meshgrid(X, Y)\n",
        "\n",
        "    Z = np.array(Z_list).T\n",
        "\n",
        "    ax.plot_wireframe(X, Y, Z)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP9iwGTGNFnm",
        "colab_type": "text"
      },
      "source": [
        "## 6. Function : plot_q_val(agent, usable_ace)\n",
        "최적 Policy를 시각화하기 위한 함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIjr46PDNFno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_action(agent: Agent, usable_ace=True):\n",
        "    hands = set()\n",
        "    dealer_show = set()\n",
        "\n",
        "    for state, action in sorted(agent.Q_table.keys()):\n",
        "        hands.add(state[0])\n",
        "        dealer_show.add(state[2])\n",
        "\n",
        "    Z_list = list()\n",
        "\n",
        "    l = list()\n",
        "\n",
        "    action = False\n",
        "\n",
        "    for d in dealer_show:\n",
        "        l = []\n",
        "        for hand in hands:\n",
        "            if ((hand, usable_ace, d), True) not in agent.Q_table.keys():\n",
        "                agent.Q_table[((hand, usable_ace, d), True)] = [0, 0]\n",
        "            if ((hand, usable_ace, d), False) not in agent.Q_table.keys():\n",
        "                agent.Q_table[((hand, usable_ace, d), False)] = [0, 0]\n",
        "            l.append(0 if agent.Q_table[((hand, usable_ace, d), True)][0] > agent.Q_table[((hand, usable_ace, d), False)][0] else 1)\n",
        "        Z_list.append(l)\n",
        "\n",
        "    Y = np.array(list(hands))\n",
        "    X = np.array(list(dealer_show))\n",
        "    if usable_ace:\n",
        "        X, Y = np.meshgrid(X, Y)\n",
        "    else:\n",
        "        X, Y = np.meshgrid(X, Y)\n",
        "    Z = np.array(Z_list).T\n",
        "\n",
        "    data = Z[::-1,:]\n",
        "    data = np.append(data, [0 for _ in range(Z[0].shape[0])]).reshape(-1,Z[0].shape[0])\n",
        "    \n",
        "    cmap=cm.coolwarm\n",
        "    plt.imshow(data, cmap=cmap, extent=[1,11,11,21])\n",
        "\n",
        "    plt.show()\n",
        "    return X, Y, Z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "kjzBx00rNFnu",
        "colab_type": "text"
      },
      "source": [
        "## 7. 최적 Policy 학습\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "siM8qzGeNFnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "deck = Deck()\n",
        "dealer = Dealer()\n",
        "agent = Agent()\n",
        "mc_es = MonteCarlo()\n",
        "\n",
        "mc_es.train(dealer, agent, deck, it=1000000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvvuYtwxNFn0",
        "colab_type": "text"
      },
      "source": [
        "## 8. 학습 결과 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pkbXSIPNFn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Q-value with Usable Ace\")\n",
        "plot_q_val(agent, usable_ace=True)\n",
        "print(\"Q-value without Usable Ace\")\n",
        "plot_q_val(agent, usable_ace=False)\n",
        "print(\"Optimal policy with Usable Ace : (RED : STICK, BLUE : HIT)\")\n",
        "usable_result = plot_action(agent, usable_ace=True)\n",
        "print(\"Optimal policy without Usable Ace : (RED : STICK, BLUE : HIT)\")\n",
        "no_usable_result = plot_action(agent, usable_ace=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz-wYhDtNFn8",
        "colab_type": "text"
      },
      "source": [
        "----\n",
        "## 9. 과제\n",
        "\n",
        "### 목표 \n",
        "위 코드로부터 최적 Policy를 학습시킨 Agent를 사용하여, 블랙잭 게임을 시뮬레이션한 뒤 승률을 계산.\n",
        "\n",
        "### 초기 설정\n",
        " - 플레이어는 1,000,000번의 에피소드로부터 최적 Policy를 학습\n",
        " - 플레이어의 초기 자금 : 10,000 달러\n",
        " - 플레이어는 게임 참가 시 10 달러를 지불, 결과에 따라 금액을 획득\n",
        "     - 승리 시 : 20 달러 획득\n",
        "     - 무승부 시 : 10 달러 획득\n",
        "     - 패배 시 : 0 달러 획득\n",
        "\n",
        "### 요구사항\n",
        " - 플레이어와 딜러가 1,000번의 게임을 진행\n",
        " - 1,000번의 게임 후 플레이어의 승률을 계산\n",
        " - 매 게임 별 플레이어의 소지금 변화를 그래프로 시각화\n",
        "\n",
        "#### * HINT : MonteCarlo 클래스의 generate_episode(), train() 메소드를 참고할 것\n",
        "----\n",
        "### (과제 1) 1,000번의 게임 진행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycjmHdGkNFn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############### 코드 작성 ##############################\n",
        "player_money = 10000\n",
        "win_count = 0\n",
        "lose_count = 0\n",
        "history = [10000]\n",
        "\n",
        "for i in range(1000):\n",
        "  episode = mc_es.generate_episode(dealer,agent,deck)\n",
        "\n",
        "  result = episode[-1][-1]\n",
        "  player_money += (10 * result)\n",
        "  history.append(player_money)\n",
        "  if result == 1:\n",
        "    win_count += 1\n",
        "  elif result == -1:\n",
        "    lose_count += 1\n",
        "########################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc_H1j3sNFoE",
        "colab_type": "text"
      },
      "source": [
        "### (과제 2) 플레이어의 승률을 계산"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTrl6H3ENFoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############### 코드 작성 ##############################\n",
        "print((win_count/(lose_count + win_count))*100)\n",
        "########################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR39kavPNFoN",
        "colab_type": "text"
      },
      "source": [
        "### (과제 3) 플레이어의 소지금 변화를 그래프로 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VVf4x75NFoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############### 코드 작성 ##############################\n",
        "fig = plt.figure()\n",
        "graph = fig.add_subplot(111)\n",
        "graph.plot(range(len(history)), history, label='GAME_MONEY', color='darkred')\n",
        "plt.xlabel('GAME_NUM')\n",
        "plt.ylabel('GAME_MONEY')\n",
        "graph.grid(linestyle='--', color='lavender')\n",
        "########################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdPRLYVTNFoU",
        "colab_type": "text"
      },
      "source": [
        "## 추가 과제 : 플레이어의 승률 높이기\n",
        "State를 수정하여 승률이 더 높은 policy를 찾기\n",
        "\n",
        "기존 코드의 수정 필요\n",
        " - 딜러는 게임이 끝났을 때, 남은 카드의 수를 확인\n",
        " - 15장 이상이라면 해당 덱을 다음 게임에서 그대로 사용\n",
        " - 15장 미만이라면 52장의 셔플된 새로운 카드를 기존 덱에 추가\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzkt5PGSNFoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############### 코드 작성 ##############################\n",
        "class Deck_develop(Deck):\n",
        "    def reset(self):\n",
        "        deck = [2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 11]\n",
        "        self.card_deck = self.card_deck + (deck * 4)\n",
        "        self.shuffle()\n",
        "\n",
        "class MonteCarlo_develope(MonteCarlo):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.left_card = 52\n",
        "        self.card_count = 0\n",
        "\n",
        "    def generate_episode(self, dealer: Dealer, agent: Agent, deck: Deck):\n",
        "        \"\"\"\n",
        "        하나의 에피소드(게임)를 생성함\n",
        "        :param dealer:\n",
        "        :param agent:\n",
        "        :param deck:\n",
        "        :return:  \n",
        "        \"\"\"\n",
        "        add_count = [0,-1,1,1,1,1,1,0,0,0,-1,-1]\n",
        "\n",
        "        # 카드 덱, 딜러, Agent를 초기화\n",
        "        if self.left_card < 15:\n",
        "            deck.reset()\n",
        "            self.left_card += 52\n",
        "\n",
        "        dealer.reset()\n",
        "        agent.reset()\n",
        "\n",
        "        agent.hit(deck)\n",
        "        agent.hit(deck)\n",
        "\n",
        "        dealer.hit(deck)\n",
        "        dealer.hit(deck)\n",
        "\n",
        "        done = False    # 에피소드의 종료 여부\n",
        "\n",
        "        episode = list()    # 에피소드\n",
        "        \n",
        "        \"\"\"\n",
        "        dealer.show()함수는 random choice인데\n",
        "        에피소드 내에서 여러번 호출되면 \n",
        "        보여지는 dealer의 카드가 바뀔 가능성이 있기때문에\n",
        "        따로 변수 선언을 통해 카드를 고정\n",
        "        \"\"\"\n",
        "        dealer_card = dealer.show()\n",
        "\n",
        "        while not done:\n",
        "            # 에피소드가 끝날 때까지 State, Action, Reward를 생성\n",
        "            sums = agent.calculate_sum()\n",
        "            if sums < 12:\n",
        "                agent.hit(deck)\n",
        "                continue\n",
        "\n",
        "            card_count_game = self.card_count\n",
        "            for card in agent.hands:\n",
        "              card_count_game += add_count[card]\n",
        "            card_count_game += add_count[dealer_card]\n",
        "\n",
        "            state = (sums, bool(agent.usable_ace), dealer_card, card_count_game)\n",
        "\n",
        "            ########   Exploring Start ~!!!!!!!!! : \n",
        "            #if len(episode) == 0:       # 첫번째 State 일 때는 무작위 Action 선택\n",
        "            #    action =agent.random_action()\n",
        "            #else:                       # 그 외에는 Q 테이블에서 큰 값을 갖는 Action 선택\n",
        "            #    action = agent.policy(state)\n",
        "\n",
        "            action = agent.policy(state)\n",
        "            \n",
        "            done, reward = dealer.observation(action, agent, deck)  # 에피소드 종료 여부, Reward 계산\n",
        "            \n",
        "            # 생성된 State, Action, Reward를 에피소드에 추가\n",
        "            episode.append([state, action, reward])\n",
        "\n",
        "        for card in agent.hands:\n",
        "            self.left_card -= 1\n",
        "            self.card_count += add_count[card]\n",
        "        for card in dealer.hands:\n",
        "            self.left_card -= 1\n",
        "            self.card_count += add_count[card]\n",
        "\n",
        "        return episode\n",
        "\n",
        "deck = Deck_develop()\n",
        "dealer = Dealer()\n",
        "agent = Agent()\n",
        "mc_es = MonteCarlo_develope()\n",
        "\n",
        "mc_es.train(dealer, agent, deck, it=1000000)\n",
        "\n",
        "player_money = 10000\n",
        "win_count = 0\n",
        "lose_count = 0\n",
        "history = [10000]\n",
        "\n",
        "for i in range(1000):\n",
        "  episode = mc_es.generate_episode(dealer,agent,deck)\n",
        "  betting_money = 100 * (1 + agent.Q_table[(episode[-1][0], episode[-1][1])][0])\n",
        "  result = episode[-1][-1]\n",
        "  player_money += (betting_money * result)\n",
        "  history.append(player_money)\n",
        "  if result == 1:\n",
        "    win_count += 1\n",
        "  elif result == -1:\n",
        "    lose_count += 1\n",
        "\n",
        "print(\"승률 : \",(win_count/(win_count+lose_count))*100)\n",
        "\n",
        "fig = plt.figure()\n",
        "graph = fig.add_subplot(111)\n",
        "graph.plot(range(len(history)), history, label='GAME_MONEY', color='darkred')\n",
        "plt.xlabel('GAME_NUM')\n",
        "plt.ylabel('GAME_MONEY')\n",
        "graph.grid(linestyle='--', color='lavender')\n",
        "########################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qpo84D6YNFoZ",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### 참고자료\n",
        "[1] Reinforcement Learning - An Introduction (Richard S. Sutton, Andreow G. Barto), 2nd edition, p.120-124 https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf <br>\n",
        "[2] matplotlib.pyplot.plot Documentation, https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html <br>\n",
        "[3] matplotlib.pyplot.plot Examples https://matplotlib.org/tutorials/introductory/pyplot.html"
      ]
    }
  ]
}